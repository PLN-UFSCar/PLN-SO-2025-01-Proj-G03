{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f516a953-3599-45de-9ed4-3286ef8b4383",
   "metadata": {},
   "source": [
    "# Detecção de sarcasmo\n",
    "\n",
    "O objetivo deste notebook é desenvolver um módulo de detecção automática de sarcasmo em textos em português, mais especificamente notícias. Para isso, foi utilizado uma base de dados composta por notícias de três grandes sites brasileiros, composta por notícias sarcásticas e não sarcásticas. Para a criação do módulo, foram criados dois modelos classificadores que utilizam metodologias diferentes: Uso de algoritmos clássicos de Machine Learning + representação vetorial estática, e Fine-tuning de um modelo transformers multi-língua\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0543dc3d-2d0f-4683-a310-eb4026fd5661",
   "metadata": {},
   "source": [
    "## Descrição da estrutura e características do data set\n",
    "\n",
    "A base de dados foi retirada do repositório [PLNCrawler](https://github.com/schuberty/PLNCrawler), e é estruturada originalmente em três arquivos JSON, que correspondem a cada site de notícias de onde as notícias foram extraídas:\n",
    "- Sensacionalista: 5006 notícias sarcásticas\n",
    "- Estadão: 11272 notícias não sarcásticas\n",
    "- Revista Piauí (seção Herald): 2216 notícias sarcásticas\n",
    "\n",
    "Cada arquivo possui os seguintes campos para cada notícia:\n",
    "- is_sarcastic (ou is_sarcasm): booleano, representa o rótulo/label da notícia (sarcástica ou não)\n",
    "- article_link: string, contem a URL de onde a notícia foi extraída\n",
    "- headline: string, contem o título da notícia\n",
    "- text: string, contem o texto da notícia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73077a2e",
   "metadata": {},
   "source": [
    "Carrega as bases de cada site em formato DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d55fb6-2d0c-4579-b677-7b413a736dde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2020/10/15/...</td>\n",
       "      <td>10 desculpas para o dinheiro entre as nádegas ...</td>\n",
       "      <td>O vice-líder do governo Bolsonaro, o senador C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2020/10/14/...</td>\n",
       "      <td>Fora Bolsonaro, ninguém gostou da advertência ...</td>\n",
       "      <td>A jogadora de vôlei de praia Carol Solberg foi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2020/10/10/...</td>\n",
       "      <td>Bolsonaro diz que a corrupção acabou mas amanh...</td>\n",
       "      <td>O presidente Jair Bolsonaro surpreendeu todo o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2020/10/10/...</td>\n",
       "      <td>Homem machuca o cérebro tentando entender fala...</td>\n",
       "      <td>Boi bombeiro. Boi. Bombeiro. BOI BOMBEIRO. bOi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2020/10/08/...</td>\n",
       "      <td>Checamos: Bolsonaro tem 89 mil motivos para di...</td>\n",
       "      <td>O presidente Jair Bolsonaro surpreendeu todo o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2009/05/08/...</td>\n",
       "      <td>Gripe suína chega ao Brasil e é assaltada em C...</td>\n",
       "      <td>Mais tarde, já relaxada, a epidemia almoçou na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2009/05/05/...</td>\n",
       "      <td>Casamento terá mesma lei do Código de Defesa d...</td>\n",
       "      <td>“Quando você compra um produto, pode trocar. U...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2009/05/04/...</td>\n",
       "      <td>Saci passa para medicina pelo sistema de cotas</td>\n",
       "      <td>Saci rebateu as críticas de que o sistema de c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2009/05/01/...</td>\n",
       "      <td>Táxis do Rio terão bandeira 3 para áreas viole...</td>\n",
       "      <td>A medida foi acertada entre a prefeitura do Ri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5005</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2009/04/28/...</td>\n",
       "      <td>Empresa lança armadura para cães no Rio</td>\n",
       "      <td>“Estamos introduzindo um conceito e, para isso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5006 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      is_sarcastic                                       article_link  \\\n",
       "0             True  https://www.sensacionalista.com.br/2020/10/15/...   \n",
       "1             True  https://www.sensacionalista.com.br/2020/10/14/...   \n",
       "2             True  https://www.sensacionalista.com.br/2020/10/10/...   \n",
       "3             True  https://www.sensacionalista.com.br/2020/10/10/...   \n",
       "4             True  https://www.sensacionalista.com.br/2020/10/08/...   \n",
       "...            ...                                                ...   \n",
       "5001          True  https://www.sensacionalista.com.br/2009/05/08/...   \n",
       "5002          True  https://www.sensacionalista.com.br/2009/05/05/...   \n",
       "5003          True  https://www.sensacionalista.com.br/2009/05/04/...   \n",
       "5004          True  https://www.sensacionalista.com.br/2009/05/01/...   \n",
       "5005          True  https://www.sensacionalista.com.br/2009/04/28/...   \n",
       "\n",
       "                                               headline  \\\n",
       "0     10 desculpas para o dinheiro entre as nádegas ...   \n",
       "1     Fora Bolsonaro, ninguém gostou da advertência ...   \n",
       "2     Bolsonaro diz que a corrupção acabou mas amanh...   \n",
       "3     Homem machuca o cérebro tentando entender fala...   \n",
       "4     Checamos: Bolsonaro tem 89 mil motivos para di...   \n",
       "...                                                 ...   \n",
       "5001  Gripe suína chega ao Brasil e é assaltada em C...   \n",
       "5002  Casamento terá mesma lei do Código de Defesa d...   \n",
       "5003     Saci passa para medicina pelo sistema de cotas   \n",
       "5004  Táxis do Rio terão bandeira 3 para áreas viole...   \n",
       "5005            Empresa lança armadura para cães no Rio   \n",
       "\n",
       "                                                   text  \n",
       "0     O vice-líder do governo Bolsonaro, o senador C...  \n",
       "1     A jogadora de vôlei de praia Carol Solberg foi...  \n",
       "2     O presidente Jair Bolsonaro surpreendeu todo o...  \n",
       "3     Boi bombeiro. Boi. Bombeiro. BOI BOMBEIRO. bOi...  \n",
       "4     O presidente Jair Bolsonaro surpreendeu todo o...  \n",
       "...                                                 ...  \n",
       "5001  Mais tarde, já relaxada, a epidemia almoçou na...  \n",
       "5002  “Quando você compra um produto, pode trocar. U...  \n",
       "5003  Saci rebateu as críticas de que o sistema de c...  \n",
       "5004  A medida foi acertada entre a prefeitura do Ri...  \n",
       "5005  “Estamos introduzindo um conceito e, para isso...  \n",
       "\n",
       "[5006 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>PF abre inquérito para investigar negócios do ...</td>\n",
       "      <td>A Polícia Federal abriu nesta segunda-feira, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>Marco Aurélio adota rito abreviado e manda açã...</td>\n",
       "      <td>O ministro Marco Aurélio Mello, do Supremo Tri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>PF prende quatro no Aeroporto de Guarulhos com...</td>\n",
       "      <td>A Polícia Federal prendeu na noite desta segun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>Entenda o que está em jogo com os recursos de ...</td>\n",
       "      <td>Caso conceda nesta terça-feira, 16, decisões f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>Existe uma terceira via?</td>\n",
       "      <td>Todo extremismo parece perigoso. Conduz ao fan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11267</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>Ministério Público obtém acordo entre grupos a...</td>\n",
       "      <td>O Ministério Público de São Paulo (MP-SP) cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11268</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>PF pega R$ 750 mil em caixa térmica na casa do...</td>\n",
       "      <td>A Polícia Federal apreendeu quase R$ 750 mil n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11269</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>Ninguém ouviu</td>\n",
       "      <td>Homens negros nascem em sua maioria nas regiõe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11270</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>Uma aventura jurídica</td>\n",
       "      <td>Segundo o Correio Braziliense, em seu site no ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11271</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>Procurador solta puns durante  sessão virtual ...</td>\n",
       "      <td>Uma cena inusitada aconteceu durante uma sessã...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11272 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_sarcastic                                       article_link  \\\n",
       "0             False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "1             False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "2             False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "3             False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "4             False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "...             ...                                                ...   \n",
       "11267         False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "11268         False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "11269         False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "11270         False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "11271         False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "\n",
       "                                                headline  \\\n",
       "0      PF abre inquérito para investigar negócios do ...   \n",
       "1      Marco Aurélio adota rito abreviado e manda açã...   \n",
       "2      PF prende quatro no Aeroporto de Guarulhos com...   \n",
       "3      Entenda o que está em jogo com os recursos de ...   \n",
       "4                               Existe uma terceira via?   \n",
       "...                                                  ...   \n",
       "11267  Ministério Público obtém acordo entre grupos a...   \n",
       "11268  PF pega R$ 750 mil em caixa térmica na casa do...   \n",
       "11269                                      Ninguém ouviu   \n",
       "11270                              Uma aventura jurídica   \n",
       "11271  Procurador solta puns durante  sessão virtual ...   \n",
       "\n",
       "                                                    text  \n",
       "0      A Polícia Federal abriu nesta segunda-feira, 1...  \n",
       "1      O ministro Marco Aurélio Mello, do Supremo Tri...  \n",
       "2      A Polícia Federal prendeu na noite desta segun...  \n",
       "3      Caso conceda nesta terça-feira, 16, decisões f...  \n",
       "4      Todo extremismo parece perigoso. Conduz ao fan...  \n",
       "...                                                  ...  \n",
       "11267  O Ministério Público de São Paulo (MP-SP) cons...  \n",
       "11268  A Polícia Federal apreendeu quase R$ 750 mil n...  \n",
       "11269  Homens negros nascem em sua maioria nas regiõe...  \n",
       "11270  Segundo o Correio Braziliense, em seu site no ...  \n",
       "11271  Uma cena inusitada aconteceu durante uma sessã...  \n",
       "\n",
       "[11272 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2014/10/...</td>\n",
       "      <td>Petição exige o impeachment de Lula</td>\n",
       "      <td>BRAZIL – Centenas de cidadãos de bem, que prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2011/04/...</td>\n",
       "      <td>Reforma política sai antes da Olimpíada, garan...</td>\n",
       "      <td>SÃO LUÍS – O presidente do Senado José Sarney ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2011/04/...</td>\n",
       "      <td>Papa barra canonização de José Alencar</td>\n",
       "      <td>VATICANO – O papa Bento XVI protestou ontem co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2012/07/...</td>\n",
       "      <td>PIB brasileiro cresce a taxas mais elevadas qu...</td>\n",
       "      <td>SÃO BERNARDO – Pesquisadores da CUT cruzaram v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2021/02/...</td>\n",
       "      <td>Banco Mundial teme receber mais um membro do g...</td>\n",
       "      <td>FREAKONOMICS – Pânico nas Bolsas de Nova York,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2211</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2018/05/...</td>\n",
       "      <td>Após pacificar Coreias, Kim Jong-un quer unifi...</td>\n",
       "      <td>LÍNGUA DO K – “Será o animal político mais sex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2212</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2013/06/...</td>\n",
       "      <td>Neymar cai cinco vezes no gramado em apresenta...</td>\n",
       "      <td>CAMP NOU – Em cerimônia que reuniu globos da m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2213</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2017/01/...</td>\n",
       "      <td>Temer indica Rubens Barrichello como novo rela...</td>\n",
       "      <td>INTERLAGOS – Comprometido em dar celeridade às...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2214</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2013/06/...</td>\n",
       "      <td>Casa Branca investiga a função de Hulk na seleção</td>\n",
       "      <td>PENTÁGONO – Após ouvir centenas de conversas e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2215</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2018/04/...</td>\n",
       "      <td>Joaquim Barbosa e Marina Silva estão em treina...</td>\n",
       "      <td>VÁCUO – “São seis encontros por semana. É uma ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2216 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      is_sarcastic                                       article_link  \\\n",
       "0             True  https://piaui.folha.uol.com.br/herald/2014/10/...   \n",
       "1             True  https://piaui.folha.uol.com.br/herald/2011/04/...   \n",
       "2             True  https://piaui.folha.uol.com.br/herald/2011/04/...   \n",
       "3             True  https://piaui.folha.uol.com.br/herald/2012/07/...   \n",
       "4             True  https://piaui.folha.uol.com.br/herald/2021/02/...   \n",
       "...            ...                                                ...   \n",
       "2211          True  https://piaui.folha.uol.com.br/herald/2018/05/...   \n",
       "2212          True  https://piaui.folha.uol.com.br/herald/2013/06/...   \n",
       "2213          True  https://piaui.folha.uol.com.br/herald/2017/01/...   \n",
       "2214          True  https://piaui.folha.uol.com.br/herald/2013/06/...   \n",
       "2215          True  https://piaui.folha.uol.com.br/herald/2018/04/...   \n",
       "\n",
       "                                               headline  \\\n",
       "0                   Petição exige o impeachment de Lula   \n",
       "1     Reforma política sai antes da Olimpíada, garan...   \n",
       "2                Papa barra canonização de José Alencar   \n",
       "3     PIB brasileiro cresce a taxas mais elevadas qu...   \n",
       "4     Banco Mundial teme receber mais um membro do g...   \n",
       "...                                                 ...   \n",
       "2211  Após pacificar Coreias, Kim Jong-un quer unifi...   \n",
       "2212  Neymar cai cinco vezes no gramado em apresenta...   \n",
       "2213  Temer indica Rubens Barrichello como novo rela...   \n",
       "2214  Casa Branca investiga a função de Hulk na seleção   \n",
       "2215  Joaquim Barbosa e Marina Silva estão em treina...   \n",
       "\n",
       "                                                   text  \n",
       "0     BRAZIL – Centenas de cidadãos de bem, que prod...  \n",
       "1     SÃO LUÍS – O presidente do Senado José Sarney ...  \n",
       "2     VATICANO – O papa Bento XVI protestou ontem co...  \n",
       "3     SÃO BERNARDO – Pesquisadores da CUT cruzaram v...  \n",
       "4     FREAKONOMICS – Pânico nas Bolsas de Nova York,...  \n",
       "...                                                 ...  \n",
       "2211  LÍNGUA DO K – “Será o animal político mais sex...  \n",
       "2212  CAMP NOU – Em cerimônia que reuniu globos da m...  \n",
       "2213  INTERLAGOS – Comprometido em dar celeridade às...  \n",
       "2214  PENTÁGONO – Após ouvir centenas de conversas e...  \n",
       "2215  VÁCUO – “São seis encontros por semana. É uma ...  \n",
       "\n",
       "[2216 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scripts.scripts import get_df_sensacionalista\n",
    "from scripts.scripts import get_df_estadao\n",
    "from scripts.scripts import get_df_the_piaui_herald\n",
    "\n",
    "# Carrega o arquivo em um DataFrame\n",
    "df_sensacionalista = get_df_sensacionalista()\n",
    "df_estadao = get_df_estadao()\n",
    "df_piaui = get_df_the_piaui_herald()\n",
    "df_piaui = df_piaui.rename(columns={'is_sarcasm': 'is_sarcastic'}) # Renomeia a coluna para igualizar com os outros DataFrames\n",
    "\n",
    "display(df_sensacionalista)\n",
    "display(df_estadao)\n",
    "display(df_piaui)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0763876-e34b-4bf6-bdeb-65d1eaa8178e",
   "metadata": {},
   "source": [
    "Faz a união das três bases em um só DataFrame de forma equilibrada, mantendo 50% de notícias sarcásticas e 50% de não sarcásticas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1960f4cd-3e68-4cfe-a454-53523fe11614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras sarcásticas: 7222\n",
      "Número de amostras não sarcásticas: 7222\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2013/05/24/...</td>\n",
       "      <td>Homem ganha Iphone em Quiz mas apanha da mulhe...</td>\n",
       "      <td>“Perdi a mulher, mas pelo menos estou com um i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2017/07/20/...</td>\n",
       "      <td>Temer prepara carta para Maia inspirado em BO ...</td>\n",
       "      <td>Um boletim de ocorrência registrado no Mato Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2015/07/17/...</td>\n",
       "      <td>Provas: PF diz que Cunha comprou 450 cuecas no...</td>\n",
       "      <td>Começam a aparecer indícios que complicam a vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2015/05/...</td>\n",
       "      <td>Tesoureiro do PT fará pronunciamento em cadeia...</td>\n",
       "      <td>PAVILHÃO 13 – Em sintonia com a propaganda do ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2015/09/03/...</td>\n",
       "      <td>Empresa demite todos os jornalistas e lança jo...</td>\n",
       "      <td>A crise econômica fez mais uma vítima na impre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14439</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2018/01/...</td>\n",
       "      <td>Ibama declara que corrupção está extinta no Br...</td>\n",
       "      <td>RESERVA NATURAL JOSÉ SARNEY – “A corrupção sai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14440</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/fausto-m...</td>\n",
       "      <td>Lewandowski prorroga medida que prevê aval da ...</td>\n",
       "      <td>No penúltimo dia do ano, o ministro Ricardo Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14441</th>\n",
       "      <td>True</td>\n",
       "      <td>https://piaui.folha.uol.com.br/herald/2011/08/...</td>\n",
       "      <td>Bilhete de Gil à diarista é considerado incomp...</td>\n",
       "      <td>SALVADOR – “Socorro, por favor deixa um guisad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14442</th>\n",
       "      <td>True</td>\n",
       "      <td>https://www.sensacionalista.com.br/2017/08/17/...</td>\n",
       "      <td>Língua Portuguesa ganhará novos adjetivos para...</td>\n",
       "      <td>Os acontecimentos recentes do Brasil deixaram ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>False</td>\n",
       "      <td>https://politica.estadao.com.br/blogs/neumanne...</td>\n",
       "      <td>Bandido solto por Marco Aurélio fugiu</td>\n",
       "      <td>1 – Ultrapassando Rosa Weber, relatora da Oper...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14444 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_sarcastic                                       article_link  \\\n",
       "0              True  https://www.sensacionalista.com.br/2013/05/24/...   \n",
       "1              True  https://www.sensacionalista.com.br/2017/07/20/...   \n",
       "2              True  https://www.sensacionalista.com.br/2015/07/17/...   \n",
       "3              True  https://piaui.folha.uol.com.br/herald/2015/05/...   \n",
       "4              True  https://www.sensacionalista.com.br/2015/09/03/...   \n",
       "...             ...                                                ...   \n",
       "14439          True  https://piaui.folha.uol.com.br/herald/2018/01/...   \n",
       "14440         False  https://politica.estadao.com.br/blogs/fausto-m...   \n",
       "14441          True  https://piaui.folha.uol.com.br/herald/2011/08/...   \n",
       "14442          True  https://www.sensacionalista.com.br/2017/08/17/...   \n",
       "14443         False  https://politica.estadao.com.br/blogs/neumanne...   \n",
       "\n",
       "                                                headline  \\\n",
       "0      Homem ganha Iphone em Quiz mas apanha da mulhe...   \n",
       "1      Temer prepara carta para Maia inspirado em BO ...   \n",
       "2      Provas: PF diz que Cunha comprou 450 cuecas no...   \n",
       "3      Tesoureiro do PT fará pronunciamento em cadeia...   \n",
       "4      Empresa demite todos os jornalistas e lança jo...   \n",
       "...                                                  ...   \n",
       "14439  Ibama declara que corrupção está extinta no Br...   \n",
       "14440  Lewandowski prorroga medida que prevê aval da ...   \n",
       "14441  Bilhete de Gil à diarista é considerado incomp...   \n",
       "14442  Língua Portuguesa ganhará novos adjetivos para...   \n",
       "14443              Bandido solto por Marco Aurélio fugiu   \n",
       "\n",
       "                                                    text  \n",
       "0      “Perdi a mulher, mas pelo menos estou com um i...  \n",
       "1      Um boletim de ocorrência registrado no Mato Gr...  \n",
       "2      Começam a aparecer indícios que complicam a vi...  \n",
       "3      PAVILHÃO 13 – Em sintonia com a propaganda do ...  \n",
       "4      A crise econômica fez mais uma vítima na impre...  \n",
       "...                                                  ...  \n",
       "14439  RESERVA NATURAL JOSÉ SARNEY – “A corrupção sai...  \n",
       "14440  No penúltimo dia do ano, o ministro Ricardo Le...  \n",
       "14441  SALVADOR – “Socorro, por favor deixa um guisad...  \n",
       "14442  Os acontecimentos recentes do Brasil deixaram ...  \n",
       "14443  1 – Ultrapassando Rosa Weber, relatora da Oper...  \n",
       "\n",
       "[14444 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Unir os 3 datasets\n",
    "from scripts.scripts import merge_dfs\n",
    "\n",
    "df = merge_dfs(df_sensacionalista, df_estadao, df_piaui)\n",
    "\n",
    "num_sarcastic = df['is_sarcastic'].sum()\n",
    "\n",
    "print(f'Número de amostras sarcásticas: {num_sarcastic}')\n",
    "print(f'Número de amostras não sarcásticas: {len(df) - num_sarcastic}')\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5090085-21ee-4fde-a10a-c1176905a9b1",
   "metadata": {},
   "source": [
    "# Pré processamento\n",
    "\n",
    "É importante pontuar que alguns recursos de linguagem que são removidos ou normalizados durante as etapas tradicionais de pré-processamento têm influencia na classificação de ironia em textos.\n",
    "Por exemplo, sinais de pontuação podem indicar ironia. Por isso, é um parâmetro do pré-processamento remover ou não esse recurso.\n",
    "\n",
    "Sabendo disso, podem ser passados parâmetros opcionais para a função de pré-processamento que aplicam ou não a transformação.\n",
    "\n",
    "## Stemming e lemmatization\n",
    "\n",
    "> \"Stemming or lemmatization reduces words to their root form (e.g., \"running\" becomes \"run\"), making it easier to analyze language by grouping different forms of the same word.\" Fonte: https://www.ibm.com/think/topics/natural-language-processing\n",
    "\n",
    "O processo de stemming e lemmatization são opcionais, mas ambos nunca podem ser aplicados juntos porque eles têm o mesmo propósito com abordagens diferentes.\n",
    "Dessa forma, se ambos forem ativados só o **lemmatization** será aplicado (por ser mais semântico).\n",
    "\n",
    "### Fontes para o pré-processamento:\n",
    "\n",
    "1. [Orientações principais](https://github.com/sharadpatell/Text_preprocessing_steps_for_NLP/blob/main/Text_preprocessing_steps_for_NLP.ipynb) que auxiliaram no passo a passo do pré-processamento.\n",
    "2. FACELI, K. et al. Inteligência Artificial Uma Abordagem de Aprendizado de Máquina. 2o edição ed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14d0771-419d-4afa-95e5-6eeb0b9bf401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5387a47-0606-48fa-ae53-64d9faf76ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[homem, ganha, iphone, quiz, apanha, mulher, t...</td>\n",
       "      <td>[perdi, mulher, menos, iphone, novo, hora, dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[temer, prepara, carta, maia, inspirado, bo, h...</td>\n",
       "      <td>[boletim, ocorrência, registrado, mato, grosso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>[provas, pf, diz, cunha, comprou, cuecas, ano,...</td>\n",
       "      <td>[começam, aparecer, indícios, complicam, vida,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>[tesoureiro, pt, fará, pronunciamento, cadeia,...</td>\n",
       "      <td>[pavilhão, sintonia, propaganda, pt, vai, ar, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>[empresa, demite, todos, jornalistas, lança, j...</td>\n",
       "      <td>[crise, econômica, fez, vítima, imprensa, dona...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14439</th>\n",
       "      <td>1</td>\n",
       "      <td>[ibama, declara, corrupção, extinta, brasil]</td>\n",
       "      <td>[reserva, natural, josé, sarney, corrupção, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14440</th>\n",
       "      <td>0</td>\n",
       "      <td>[lewandowski, prorroga, medida, prevê, aval, a...</td>\n",
       "      <td>[penúltimo, dia, ano, ministro, ricardo, lewan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14441</th>\n",
       "      <td>1</td>\n",
       "      <td>[bilhete, gil, diarista, considerado, incompre...</td>\n",
       "      <td>[salvador, socorro, favor, deixa, guisadinho, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14442</th>\n",
       "      <td>1</td>\n",
       "      <td>[língua, portuguesa, ganhará, novos, adjetivos...</td>\n",
       "      <td>[acontecimentos, recentes, brasil, deixaram, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14443</th>\n",
       "      <td>0</td>\n",
       "      <td>[bandido, solto, marco, aurélio, fugiu]</td>\n",
       "      <td>[ultrapassando, rosa, weber, relatora, operaçã...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14444 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_sarcastic                                           headline  \\\n",
       "0                 1  [homem, ganha, iphone, quiz, apanha, mulher, t...   \n",
       "1                 1  [temer, prepara, carta, maia, inspirado, bo, h...   \n",
       "2                 1  [provas, pf, diz, cunha, comprou, cuecas, ano,...   \n",
       "3                 1  [tesoureiro, pt, fará, pronunciamento, cadeia,...   \n",
       "4                 1  [empresa, demite, todos, jornalistas, lança, j...   \n",
       "...             ...                                                ...   \n",
       "14439             1       [ibama, declara, corrupção, extinta, brasil]   \n",
       "14440             0  [lewandowski, prorroga, medida, prevê, aval, a...   \n",
       "14441             1  [bilhete, gil, diarista, considerado, incompre...   \n",
       "14442             1  [língua, portuguesa, ganhará, novos, adjetivos...   \n",
       "14443             0            [bandido, solto, marco, aurélio, fugiu]   \n",
       "\n",
       "                                                    text  \n",
       "0      [perdi, mulher, menos, iphone, novo, hora, dis...  \n",
       "1      [boletim, ocorrência, registrado, mato, grosso...  \n",
       "2      [começam, aparecer, indícios, complicam, vida,...  \n",
       "3      [pavilhão, sintonia, propaganda, pt, vai, ar, ...  \n",
       "4      [crise, econômica, fez, vítima, imprensa, dona...  \n",
       "...                                                  ...  \n",
       "14439  [reserva, natural, josé, sarney, corrupção, sa...  \n",
       "14440  [penúltimo, dia, ano, ministro, ricardo, lewan...  \n",
       "14441  [salvador, socorro, favor, deixa, guisadinho, ...  \n",
       "14442  [acontecimentos, recentes, brasil, deixaram, b...  \n",
       "14443  [ultrapassando, rosa, weber, relatora, operaçã...  \n",
       "\n",
       "[14444 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scripts.preprocessamento import pre_processamento\n",
    "\n",
    "usar_lemmatization = False\n",
    "usar_stemming      = False\n",
    "\n",
    "df = pre_processamento(df, usar_stemming = usar_stemming, usar_lemmatization = usar_lemmatization)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7899157-7fc8-4692-891d-451fee3b3ebf",
   "metadata": {},
   "source": [
    "## Primeira abordagem para deteccção: Uso de algoritmos clássicos de Machine Learning + representação vetorial estática\n",
    "\n",
    "Para essa abordagem, o primeiro passo é criar a representação vetorial do texto, pois os computadores não interpretam os textos na linguagem do ser humano. Por isso, é necessário transformá-los para uma representação estruturada que as máquinas consigam processar.\n",
    "\n",
    "Esse tratamento do texto também faz parte da etapa de feature extraction.\n",
    "> Feature extraction is the process of converting raw text into numerical representations that machines can analyze and interpret. Fonte: https://www.ibm.com/think/topics/natural-language-processing\n",
    "\n",
    "Foi escolhido usar a ferramenta Word2Vec para a geração de vetores densos, que capturam o valor semântico das palavras e as relacionam entre sí. É ideal para tarefas de aprendizado de máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b5232b5-8124-4260-939c-c2d77e859fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "usar_word2vec = False\n",
    "usar_sequence_transformer = not usar_word2vec\n",
    "usar_sequence_transformer = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da54478",
   "metadata": {},
   "source": [
    "Aplica Word2Vec na base de dados, gerando a rede neural e retornando os embeddings para cada notícia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66652b26-5171-48e8-a22c-7e5ebe692443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.representacao_computacional import aplica_word2vec\n",
    "# modelo, embeddings = aplica_word2vec(df, nome_coluna='headline')\n",
    "\n",
    "if (usar_word2vec):\n",
    "    import subprocess\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    \n",
    "    # Salvar o dataframe temporariamente\n",
    "    df.to_parquet(\"temp_input.parquet\", engine=\"pyarrow\")  \n",
    "    \n",
    "    # Comando para ativar conda env e rodar o script\n",
    "    subprocess.run([\n",
    "        \"conda\", \"run\", \"-n\", \"word2vec_env\", \"python\",\n",
    "        \"scripts/word2vec_runner.py\", \"0\", \"temp_input.parquet\", \"text\", \"skip-gram\"\n",
    "    ])\n",
    "    \n",
    "    # Recuperar o resultado\n",
    "    with open(\"embeddings_output.pkl\", \"rb\") as f:\n",
    "        embeddings = pickle.load(f)\n",
    "\n",
    "    with open(\"indices_validos.pkl\", \"rb\") as f:\n",
    "        indices_validos = pickle.load(f)\n",
    "    \n",
    "    embeddings\n",
    "    print(embeddings[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6f7de",
   "metadata": {},
   "source": [
    "### Treinar um modelo de ML tradicional usando os embeddings Word2Vec\n",
    "\n",
    "Criados os embeddings, o próximo passo é iniciar o treinamento de um modelo de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b190d36",
   "metadata": {},
   "source": [
    "Divisão de dados de treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bb7853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (usar_word2vec):\n",
    "    import numpy as np\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Convertendo para arrays\n",
    "    X = np.array(embeddings)\n",
    "    y = df.iloc[indices_validos][\"is_sarcastic\"].astype(int).values\n",
    "    \n",
    "    print(f\"X shape: {X.shape}, y shape: {y.shape}\")\n",
    "    \n",
    "    # Confirma que estão alinhados\n",
    "    assert len(X) == len(y)\n",
    "    print(len(X), len(y))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072d462",
   "metadata": {},
   "source": [
    "Testa diversos modelos/algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "056f0bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (usar_word2vec):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.svm import SVC\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    \n",
    "    modelos = {\n",
    "        \"SVM\": SVC(kernel='linear', probability=True),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "        \"Decision Tree\": DecisionTreeClassifier(),\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    for nome, modelo in modelos.items():\n",
    "        print(f\"\\n=== {nome} ===\")\n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05040df",
   "metadata": {},
   "source": [
    "Os resultados foram similares, porém o melhor foi o algoritmo Random Forest, sendo esse o escolhido para o modelo final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e112c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (usar_word2vec):\n",
    "    import joblib\n",
    "\n",
    "    modelo = RandomForestClassifier(n_estimators=100)\n",
    "    modelo.fit(X_train, y_train)\n",
    "    y_pred = modelo.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    joblib.dump(modelo, \"modelo_word2vec.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96c879",
   "metadata": {},
   "source": [
    "### Predição de sarcasmo usando o modelo gerado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "816922de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (usar_word2vec):\n",
    "    from scripts.preprocessamento import pre_processamento_frase\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    import subprocess\n",
    "    \n",
    "    # Carrega modelo Word2Vec\n",
    "    # w2v_model = Word2Vec.load(\"modelo_word2vec.model\")\n",
    "    \n",
    "    # Carrega classificador treinado (SVM, Random Forest etc.)\n",
    "    classificador = joblib.load(\"modelo_word2vec.pkl\")\n",
    "    \n",
    "    frase = input(\"Digite um texto para análise: \")\n",
    "    \n",
    "    tokens = pre_processamento_frase(frase)\n",
    "    \n",
    "    # salvar os tokens em um arquivo CSV temporário\n",
    "    pd.DataFrame({\"tokens\": [tokens]}).to_csv(\"frase_processada.csv\", index=False)\n",
    "    \n",
    "    # Roda word2vec na frase processada\n",
    "    \n",
    "    subprocess.run([\n",
    "            \"conda\", \"run\", \"-n\", \"word2vec_env\", \"python\",\n",
    "            \"scripts/word2vec_runner.py\", \"1\"\n",
    "        ], check=True, capture_output=True)\n",
    "    \n",
    "    # Carregar os embeddings do arquivo CSV\n",
    "    vetor = pd.read_csv(\"vetor_word2vec.csv\", header=None).values\n",
    "    # print(f\"[INFO] Vetor carregado do CSV: {vetor_carregado}\")\n",
    "    \n",
    "    # vetor = vetor_medio(tokens, w2v_model)\n",
    "    \n",
    "    \n",
    "    # Previsão\n",
    "    pred = classificador.predict(vetor)\n",
    "    prob = classificador.predict_proba(vetor)[0]\n",
    "    \n",
    "    if pred[0] == 1:\n",
    "        print(f\"Sarcamo detectado (confiança: {prob[1]:.2f})\")\n",
    "    else:\n",
    "        print(f\"Sarcamo não detectado (confiança: {prob[0]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82be05-2c38-4f05-a7b1-670f0e32a00a",
   "metadata": {},
   "source": [
    "## Segunda abordagem para detecção: Fine-tuning de um modelo Sentence Transformer\n",
    "\n",
    "A segunda abordagem é composta pela escolha de um modelo Transformrers de linguagem, e a partir dele realizer um fine-tuning pra o nosso objetivo.\n",
    "\n",
    "\"Finetuning Sentence Transformer models often heavily improves the performance of the model on your use case, because each task requires a different notion of similarity.\"\n",
    "Fonte: https://sbert.net/docs/sentence_transformer/training_overview.html\n",
    "\n",
    "O modelo escolhido para o treinamento/ finetuning foi o _______ porque ______.\n",
    "A função de perda escolhida foi _____. De acordo com [essa tabela](https://sbert.net/docs/sentence_transformer/loss_overview.html), o nosso dataset é da forma (texto, texto, label)\n",
    "\n",
    "Antes da aplicação do fine tuning, é importante que o dataset esteja de acordo com a função de perda.\n",
    "\"It is important that your dataset format matches your loss function (or that you choose a loss function that matches your dataset format)\"\n",
    "\n",
    "Para textos curtos (como é o exemplo da headline), o Word2Vec funciona bem. Para textos longos (como é o caso de notícias), pode ser mais efetivo utilizar transformers como BERT.\n",
    "\n",
    "Encontrar um modelo Sequence Transformer:\n",
    "- Treinado ou adaptado para pt-BR\n",
    "- Ser fine-tuning em sentence similarity, feature extraction\n",
    "- Treinado preferenciamente em notícias\n",
    "- Usar uma arquitetura encoder compatível com sentence-transformers\n",
    "\n",
    "\n",
    "Assim foi escolhido o modelo paraphrase-multilingual-MiniLM-L12-v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a76d1",
   "metadata": {},
   "source": [
    "Carrega o modelo original e realiza o ajuste fino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7758206a-26a0-46d4-820c-426828cd0cad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if (usar_sequence_transformer):\n",
    "    import subprocess\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    print('  Carregando modelo base...')\n",
    "    # Carrega modelo base\n",
    "    modelo = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "    # modelo = SentenceTransformer(\"sentence-transformers/xlm-r-bert-base-nli-stsb-mean-tokens\")\n",
    "    \n",
    "    # Salva o modelo para ser reutilizado no subprocesso\n",
    "    modelo.save(\"modelo_temporario_transformer\")\n",
    "    \n",
    "    # Salva o DataFrame temporariamente\n",
    "    print('Salvando o DataFrame temporário...')\n",
    "    df.to_parquet(\"temp_input.parquet\")\n",
    "    print('DataFrame temporário salvo.')\n",
    "    \n",
    "    # Executa o subprocesso\n",
    "    print('Iniciando execução do subprocesso...')\n",
    "    results = subprocess.run([\n",
    "        \"conda\", \"run\", \"-n\", \"transformers_env\", \"python\", \"-u\",\n",
    "        \"scripts/fine_tuning.py\", \"temp_input.parquet\", \"modelo_temporario_transformer\"\n",
    "    ], check=True)\n",
    "    print('Execução do subprocesso finalizada.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd6544",
   "metadata": {},
   "source": [
    "Carrega o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7dbf85-6c30-4a0f-aaf8-f843a19282d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "carregar_modelo = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa0d2da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Carregando modelo e classificador...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.7.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if carregar_modelo:\n",
    "    import os\n",
    "    import joblib\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    # Caminhos dos arquivos salvos\n",
    "    MODELO_DIR = \"modelo_finetunado_sarcasmo\"\n",
    "    CLASSIFICADOR_PATH = os.path.join(MODELO_DIR, \"classificador_logreg.pkl\")\n",
    "    \n",
    "    def carregar_modelo():\n",
    "        if not os.path.exists(MODELO_DIR):\n",
    "            raise FileNotFoundError(f\"Diretório '{MODELO_DIR}' não encontrado.\")\n",
    "        if not os.path.exists(CLASSIFICADOR_PATH):\n",
    "            raise FileNotFoundError(f\"Classificador '{CLASSIFICADOR_PATH}' não encontrado.\")\n",
    "    \n",
    "        print(\"[INFO] Carregando modelo e classificador...\")\n",
    "        modelo = SentenceTransformer(MODELO_DIR)\n",
    "        classificador = joblib.load(CLASSIFICADOR_PATH)\n",
    "        return modelo, classificador\n",
    "    \n",
    "    \n",
    "    modelo, classificador = carregar_modelo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e4b9ba",
   "metadata": {},
   "source": [
    "Predição de sarcasmo usando o modelo gerado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c3d1ae-9633-481f-8aeb-4127ab8a7064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy\n",
    "\n",
    "def carregar_modelo():\n",
    "    if not os.path.exists(MODELO_DIR):\n",
    "        raise FileNotFoundError(f\"Diretório '{MODELO_DIR}' não encontrado.\")\n",
    "    if not os.path.exists(CLASSIFICADOR_PATH):\n",
    "        raise FileNotFoundError(f\"Classificador '{CLASSIFICADOR_PATH}' não encontrado.\")\n",
    "\n",
    "    print(\"[INFO] Carregando modelo e classificador...\")\n",
    "    modelo = SentenceTransformer(MODELO_DIR)\n",
    "    classificador = joblib.load(CLASSIFICADOR_PATH)\n",
    "    return modelo, classificador\n",
    "\n",
    "\n",
    "def prever_sarcasmo(frase, modelo, classificador, limiar=0.5):\n",
    "    # embedding = modelo.encode([frase], convert_to_tensor=True).cpu().numpy()\n",
    "    embedding = modelo.encode([frase], convert_to_tensor=True).cpu().tolist()\n",
    "    prob = classificador.predict_proba(embedding)[0][1]  # Probabilidade de sarcasmo\n",
    "\n",
    "    if prob >= limiar:\n",
    "        return \"Sarcasmo detectado\", prob\n",
    "    else:\n",
    "        return \"Sarcasmo não detectado\", prob\n",
    "\n",
    "\n",
    "# print(\"\\nDigite uma frase para detectar sarcasmo:\")\n",
    "\n",
    "# frase = input(\"\\n> \")\n",
    "\n",
    "# if len(frase.strip()) == 0:\n",
    "    # print(\"[ERRO] Frase vazia. Tente novamente.\")\n",
    "\n",
    "# resultado, prob = prever_sarcasmo(frase, modelo, classificador)\n",
    "# print(f\"{resultado} (confiança: {prob:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36b5de61-235d-4898-8365-e7fc081e6c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- -----------\n",
      "accelerate                1.8.1\n",
      "aiohappyeyeballs          2.6.1\n",
      "aiohttp                   3.12.13\n",
      "aiosignal                 1.3.2\n",
      "annotated-types           0.6.0\n",
      "anyio                     4.9.0\n",
      "appdirs                   1.4.4\n",
      "argon2-cffi               21.3.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "asttokens                 3.0.0\n",
      "async-lru                 2.0.4\n",
      "async-timeout             5.0.1\n",
      "attrs                     24.3.0\n",
      "babel                     2.16.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.2.0\n",
      "blis                      1.0.1\n",
      "Bottleneck                1.4.2\n",
      "brotlicffi                1.0.9.2\n",
      "catalogue                 2.0.10\n",
      "certifi                   2025.6.15\n",
      "cffi                      1.17.1\n",
      "charset-normalizer        3.3.2\n",
      "click                     8.1.8\n",
      "cloudpathlib              0.21.0\n",
      "colorama                  0.4.6\n",
      "comm                      0.2.1\n",
      "confection                0.1.5\n",
      "cymem                     2.0.6\n",
      "datasets                  2.14.4\n",
      "debugpy                   1.8.11\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "dill                      0.3.7\n",
      "exceptiongroup            1.2.0\n",
      "executing                 0.8.3\n",
      "fastjsonschema            2.20.0\n",
      "filelock                  3.18.0\n",
      "frozenlist                1.6.0\n",
      "fsspec                    2025.5.1\n",
      "h11                       0.16.0\n",
      "httpcore                  1.0.9\n",
      "httpx                     0.28.1\n",
      "httpx-ws                  0.7.2\n",
      "huggingface_hub           0.31.4\n",
      "idna                      3.7\n",
      "importlib_metadata        8.7.0\n",
      "ipykernel                 6.29.5\n",
      "ipython                   8.30.0\n",
      "ipython_pygments_lexers   1.1.1\n",
      "ipywidgets                8.1.5\n",
      "jedi                      0.19.2\n",
      "Jinja2                    3.1.6\n",
      "joblib                    1.4.2\n",
      "json5                     0.9.25\n",
      "jsonschema                4.23.0\n",
      "jsonschema-specifications 2023.7.1\n",
      "jupyter                   1.1.1\n",
      "jupyter_client            8.6.3\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.12.0\n",
      "jupyter-lsp               2.2.5\n",
      "jupyter_server            2.15.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.3.4\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.27.3\n",
      "jupyterlab_widgets        3.0.13\n",
      "langcodes                 3.3.0\n",
      "lmstudio                  1.4.1\n",
      "markdown-it-py            2.2.0\n",
      "MarkupSafe                3.0.2\n",
      "matplotlib-inline         0.1.6\n",
      "mdurl                     0.1.0\n",
      "mistune                   3.1.2\n",
      "mkl_fft                   1.3.11\n",
      "mkl_random                1.2.8\n",
      "mkl-service               2.4.0\n",
      "mpmath                    1.3.0\n",
      "msgspec                   0.19.0\n",
      "multidict                 6.6.3\n",
      "multiprocess              0.70.15\n",
      "murmurhash                1.0.12\n",
      "nbclient                  0.10.2\n",
      "nbconvert                 7.16.6\n",
      "nbformat                  5.10.4\n",
      "nest-asyncio              1.6.0\n",
      "networkx                  3.4.2\n",
      "nltk                      3.9.1\n",
      "notebook                  7.3.2\n",
      "notebook_shim             0.2.4\n",
      "numexpr                   2.10.1\n",
      "numpy                     2.2.6\n",
      "nvidia-cublas-cu12        12.6.4.1\n",
      "nvidia-cuda-cupti-cu12    12.6.80\n",
      "nvidia-cuda-nvrtc-cu12    12.6.77\n",
      "nvidia-cuda-runtime-cu12  12.6.77\n",
      "nvidia-cudnn-cu12         9.5.1.17\n",
      "nvidia-cufft-cu12         11.3.0.4\n",
      "nvidia-cufile-cu12        1.11.1.6\n",
      "nvidia-curand-cu12        10.3.7.77\n",
      "nvidia-cusolver-cu12      11.7.1.2\n",
      "nvidia-cusparse-cu12      12.5.4.2\n",
      "nvidia-cusparselt-cu12    0.6.3\n",
      "nvidia-nccl-cu12          2.26.2\n",
      "nvidia-nvjitlink-cu12     12.6.85\n",
      "nvidia-nvtx-cu12          12.6.77\n",
      "overrides                 7.4.0\n",
      "packaging                 24.2\n",
      "pandas                    2.2.3\n",
      "pandocfilters             1.5.0\n",
      "parso                     0.8.4\n",
      "pexpect                   4.9.0\n",
      "pillow                    11.1.0\n",
      "pip                       25.1\n",
      "platformdirs              4.3.7\n",
      "preshed                   3.0.9\n",
      "prometheus_client         0.21.1\n",
      "prompt-toolkit            3.0.43\n",
      "propcache                 0.3.1\n",
      "psutil                    5.9.0\n",
      "pt_core_news_sm           3.8.0\n",
      "ptyprocess                0.7.0\n",
      "pure-eval                 0.2.2\n",
      "pyarrow                   19.0.0\n",
      "pycparser                 2.21\n",
      "pydantic                  2.11.7\n",
      "pydantic_core             2.33.2\n",
      "Pygments                  2.19.1\n",
      "PyHyphen                  4.0.4\n",
      "PyQt6                     6.7.1\n",
      "PyQt6_sip                 13.9.1\n",
      "PySocks                   1.7.1\n",
      "pyspark                   3.5.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        3.2.1\n",
      "pytz                      2024.1\n",
      "PyYAML                    6.0.2\n",
      "pyzmq                     26.2.0\n",
      "qtconsole                 5.6.1\n",
      "QtPy                      2.4.1\n",
      "referencing               0.30.2\n",
      "regex                     2024.11.6\n",
      "requests                  2.32.4\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rich                      13.9.4\n",
      "rpds-py                   0.22.3\n",
      "safetensors               0.5.3\n",
      "scikit-learn              1.6.1\n",
      "scipy                     1.15.3\n",
      "Send2Trash                1.8.2\n",
      "sentence-transformers     5.0.0\n",
      "setuptools                72.1.0\n",
      "shellingham               1.5.0\n",
      "sip                       6.10.0\n",
      "six                       1.17.0\n",
      "smart-open                5.2.1\n",
      "sniffio                   1.3.0\n",
      "soupsieve                 2.5\n",
      "spacy                     3.8.2\n",
      "spacy-legacy              3.0.12\n",
      "spacy-loggers             1.0.4\n",
      "srsly                     2.5.1\n",
      "stack-data                0.2.0\n",
      "sympy                     1.14.0\n",
      "terminado                 0.17.1\n",
      "thinc                     8.3.2\n",
      "threadpoolctl             3.6.0\n",
      "tinycss2                  1.4.0\n",
      "tokenizers                0.21.0\n",
      "tomli                     2.0.1\n",
      "torch                     2.7.1\n",
      "tornado                   6.5.1\n",
      "tqdm                      4.67.1\n",
      "traitlets                 5.14.3\n",
      "transformers              4.53.1\n",
      "triton                    3.3.1\n",
      "typer                     0.9.0\n",
      "typing_extensions         4.12.2\n",
      "typing-inspection         0.4.0\n",
      "tzdata                    2025.2\n",
      "urllib3                   2.5.0\n",
      "wasabi                    0.9.1\n",
      "wcwidth                   0.2.13\n",
      "weasel                    0.4.1\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.8.0\n",
      "wheel                     0.45.1\n",
      "widgetsnbextension        4.0.13\n",
      "wn                        0.13.0\n",
      "wsproto                   1.2.0\n",
      "xxhash                    3.5.0\n",
      "yarl                      1.20.1\n",
      "zipp                      3.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d59897-d6a4-4fe1-8c07-821a51eda347",
   "metadata": {},
   "source": [
    "# Parte 2: Detecção de Ambiguidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebe50d33-17f6-4838-945a-ec91f32889d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wn in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (0.13.0)\n",
      "Requirement already satisfied: httpx in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (from wn) (0.28.1)\n",
      "Requirement already satisfied: tomli in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (from wn) (2.0.1)\n",
      "Requirement already satisfied: anyio in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (from httpx->wn) (4.9.0)\n",
      "Requirement already satisfied: certifi in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (from httpx->wn) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (from httpx->wn) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (from httpx->wn) (3.7)\n",
      "Requirement already satisfied: h11>=0.16 in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (from httpcore==1.*->httpx->wn) (0.16.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (from anyio->httpx->wn) (1.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (from anyio->httpx->wn) (1.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in /home/paula/anaconda3/envs/ironia_env/lib/python3.10/site-packages (from anyio->httpx->wn) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5955e9c-f458-4867-813f-d081b8253e07",
   "metadata": {},
   "source": [
    "# Reescrita de frase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ccbf0f7-1ff8-4b90-ba45-6389654e4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def verificarAmbiguidadePalavra(palavra, contexto):\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            \"conda\", \"run\", \"-n\", \"ambiguidade_env\", \"python\",\n",
    "            \"scripts/ambiguidade.py\", contexto, palavra\n",
    "        ],\n",
    "        check=True,\n",
    "        capture_output=True,\n",
    "        text=True  # Para já retornar string ao invés de bytes\n",
    "    )\n",
    "\n",
    "    if (result.stdout.strip() == 'None'):\n",
    "        return [False, \"\"]\n",
    "\n",
    "    return [True, result.stdout.strip()]\n",
    "\n",
    "def verificarIroniaFrase(frase):\n",
    "    if len(frase.strip()) == 0:\n",
    "        print(\"[ERRO] Frase vazia. Tente novamente.\")\n",
    "\n",
    "    print('Frase: ', frase)\n",
    "    resultado, prob = prever_sarcasmo(frase, modelo, classificador)\n",
    "    print('Resultado: ', resultado)\n",
    "\n",
    "    if resultado.strip() == 'Sarcasmo detectado':\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "761c5b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec2e281-1e6a-4048-aa20-b611568a883a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/paula/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Digite um texto para análise e reescrita (-1 para finalizar):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " oi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase:  oi\n",
      "Resultado:  Sarcasmo detectado\n",
      "\n",
      "--- Itens Detectados para o Prompt ---\n",
      "Frases Irônicas: ['oi']\n",
      "Palavras Ambíguas por Frase: {}\n",
      "------------------------------------\n",
      "Você é um assistente de reescrita de texto para pessoas com Transtorno do Espectro Autista (TEA). Seu objetivo principal é garantir que o texto seja CLARO, DIRETO E LITERAL, removendo QUALQUER ironia, sarcasmo, duplo sentido, linguagem figurada ou ambiguidade. Use um tom neutro e factual.\n",
      "\n",
      "    \n",
      "    INSTRUÇÕES E REGRAS ESSENCIAIS:\n",
      "\n",
      "    1. **REMOÇÃO DE IRONIA:**\n",
      "        - Substitua frases irônicas por suas mensagens LITERAIS e DIRETAS.\n",
      "        - EXEMPLO: 'Que dia maravilhoso' (sarcástico) -> 'O dia foi ruim' (direto).\n",
      "\n",
      "    2. **RESOLUÇÃO DE AMBIGUIDADE COM VALIDAÇÃO CONTEXTUAL:**\n",
      "        - Para as palavras listadas abaixo, uma análise contextual prévia já sugeriu um sentido específico.\n",
      "        - **SUA PRIMEIRA TAREFA É VALIDAR SE ESSA SUGESTÃO DE SENTIDO ESTÁ CORRETA.**\n",
      "        - **SE A SUGESTÃO ESTIVER CORRETA:** Use-a para reescrever a frase de forma explícita e clara, eliminando a ambiguidade.\n",
      "            - EXEMPLO: 'O banco estava fechado.' (Sugestão contextual: 'banco' -> instituição financeira) -> 'A agência bancária estava fechada.'\n",
      "        - **SE VOCÊ, COMO UM MODELO DE LINGUAGEM SUPERIOR, JULGAR QUE A SUGESTÃO ESTÁ ERRADA ou incompleta:** Ignore a sugestão e use seu próprio entendimento do contexto para clarificar a frase da melhor forma possível.\n",
      "\n",
      "    3. **CLAREZA E SIMPLICIDADE GERAL:**\n",
      "        - Use frases curtas e objetivas. Evite metáforas, ditados populares e gírias.\n",
      "\n",
      "    4. **PRESERVAÇÃO DO CONTEÚDO ORIGINAL:**\n",
      "        - Mantenha TODAS as informações essenciais. NÃO remova ou adicione fatos.\n",
      "\n",
      "    5. **RESTRIÇÃO CHAVE: NÃO ALTERE PARTES DO TEXTO QUE JÁ SÃO CLARAS.**\n",
      "       APENAS modifique os segmentos identificados ou que você julgue problemáticos.\n",
      "    \n",
      "\n",
      "    \n",
      "    **EXEMPLOS DE REESCRITA (ENTRADA -> SAÍDA DIRETAS):**\n",
      "\n",
      "    **Exemplo 1 (Ironia):**\n",
      "    ENTRADA: \"Nossa, que dia ótimo! Esqueci a carteira em casa.\"\n",
      "    SAÍDA: \"Foi um dia ruim. Eu esqueci minha carteira em casa.\"\n",
      "\n",
      "    **Exemplo 2 (Ambiguidade Validada):**\n",
      "    ENTRADA: \"Comprei uma manga para o lanche.\" (Sugestão contextual: 'manga' -> fruta)\n",
      "    SAÍDA: \"Comprei uma fruta manga para o lanche.\"\n",
      "\n",
      "    **Exemplo 3 (Ambiguidade onde o LLM pode refinar a sugestão):**\n",
      "    ENTRADA: \"A manga da camisa rasgou.\" (Sugestão contextual: 'manga' -> parte de uma roupa)\n",
      "    SAÍDA: \"A parte da camisa que cobre o braço, a manga, rasgou.\"\n",
      "\n",
      "    **Exemplo 4 (Preservar clareza, não alterar):**\n",
      "    ENTRADA: \"A água é essencial para a vida.\"\n",
      "    SAÍDA: \"A água é essencial para a vida.\"\n",
      "    \n",
      "\n",
      "    \n",
      "**ELEMENTOS ESPECÍFICOS PARA SUA ANÁLISE:**\n",
      "\n",
      "**FRASES COM POTENCIAL IRONIA:**\n",
      "1. \"oi\"\n",
      "\n",
      "\n",
      "    **TEXTO ORIGINAL A SER REESCRITO:**\n",
      "    \"oi\"\n",
      "\n",
      "    **TAREFA FINAL:**\n",
      "    Com base nas **INSTRUÇÕES E REGRAS ESSENCIAIS** e nos **EXEMPLOS**, reescreva o **TEXTO ORIGINAL A SER REESCRITO**.\n",
      "    Lembre-se de **validar o contexto** das ambiguidades sugeridas antes de fazer qualquer alteração.\n",
      "    **FORNEÇA APENAS O TEXTO REESCRITO FINAL, SEM QUALQUER TEXTO INTRODUTÓRIO OU EXPLICATIVO.**\n",
      "\n",
      "    **TEXTO REESCRITO FINAL:**\n",
      "\n",
      "--- Gerando texto com Gemini (Modelo hard-coded: gemini-2.5-flash) ---\n",
      "\n",
      "--- TEXTO REESCRITO ---\n",
      "oi\n",
      "-----------------------\n",
      "{'Flesch texto original': 163.22, 'Flesch texto reescrito': 163.22, 'Melhora na legibilidade (ou piora)': 0.0, 'Manteve o sentido?': True, 'Similaridade': np.float64(1.0000000000000002)}\n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "Digite um texto para análise e reescrita (-1 para finalizar):\n"
     ]
    }
   ],
   "source": [
    "#import string\n",
    "#import lmstudio as lms\n",
    "import google.generativeai as genai\n",
    "API_KEY = ''\n",
    "genai.configure(api_key = API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "from scripts.reescrita import frases\n",
    "from scripts.reescrita import palavras\n",
    "from scripts.reescrita import gerarPrompt\n",
    "#from scripts.reescrita import gerar_texto_com_lmstudio\n",
    "\n",
    "from scripts.avaliacao import Avaliacao\n",
    "\n",
    "# --- Loop Principal do Programa ---\n",
    "\n",
    "avaliacao = Avaliacao()\n",
    "\n",
    "while(1):\n",
    "    print(\"\\n------------------------------------------------------\\n\")\n",
    "    print(\"Digite um texto para análise e reescrita (-1 para finalizar):\")\n",
    "    texto_original = input()\n",
    "\n",
    "    if texto_original == \"-1\":\n",
    "       break\n",
    "\n",
    "    # 1. Identificação de elementos problemáticos (sarcasmo e ambiguidade)\n",
    "    palavras_ambiguas_por_frase = {}\n",
    "    frases_ironicas = []\n",
    "\n",
    "    lista_frases = frases(texto_original)\n",
    "    \n",
    "    for frase in lista_frases:\n",
    "        if verificarIroniaFrase(frase) == True:\n",
    "            frases_ironicas.append(frase)\n",
    "        \n",
    "        listapalavrasfrase = palavras(frase)\n",
    "        palavrasAmbiguasNaFrase = []\n",
    "        for palavrafrase in listapalavrasfrase:\n",
    "            resultadoambiguidade = verificarAmbiguidadePalavra(palavrafrase, frase)\n",
    "            if resultadoambiguidade[0] == True:\n",
    "                palavrasAmbiguasNaFrase.append((palavrafrase, resultadoambiguidade[1]))\n",
    "        \n",
    "        if palavrasAmbiguasNaFrase:\n",
    "            palavras_ambiguas_por_frase[frase] = palavrasAmbiguasNaFrase\n",
    "\n",
    "    \n",
    "\n",
    "    print(\"\\n--- Itens Detectados para o Prompt ---\")\n",
    "    print(f\"Frases Irônicas: {frases_ironicas}\")\n",
    "    print(f\"Palavras Ambíguas por Frase: {palavras_ambiguas_por_frase}\")\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "    # 2. Geração do Prompt Otimizado\n",
    "    prompt_final = gerarPrompt(texto_original, frases_ironicas, palavras_ambiguas_por_frase)\n",
    "    print(prompt_final)\n",
    "\n",
    "    # 3. Geração do Texto Tratado pelo LLM (GEMINI)\n",
    "    print(\"\\n--- Gerando texto com Gemini (Modelo hard-coded: gemini-2.5-flash) ---\")\n",
    "    texto_reescrito = model.generate_content(prompt_final).text\n",
    "    #texto_reescrito = gerar_texto_com_lmstudio(prompt_final)\n",
    "\n",
    "    if texto_reescrito:\n",
    "        if texto_reescrito.strip().startswith(\"TEXTO REESCRITO:\"):\n",
    "            texto_reescrito = texto_reescrito.strip()[len(\"TEXTO REESCRITO:\"):].strip()\n",
    "        \n",
    "        if texto_reescrito.startswith('\"') and texto_reescrito.endswith('\"'):\n",
    "            texto_reescrito = texto_reescrito[1:-1].strip()\n",
    "\n",
    "        print(\"\\n--- TEXTO REESCRITO ---\")\n",
    "        print(texto_reescrito)\n",
    "        print(\"-----------------------\")\n",
    "\n",
    "        print(avaliacao.avaliarReescrita(texto_original, texto_reescrito))\n",
    "    else:\n",
    "        print(\"\\nNão foi possível gerar o texto reescrito.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be501a8-6db9-4726-a28b-331d30a8e299",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ironia_env)",
   "language": "python",
   "name": "ironia_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
